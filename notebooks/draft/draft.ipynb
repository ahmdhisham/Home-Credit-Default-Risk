{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application_train table path\n",
    "APPLICATION_TRAIN_DATA_PATH = '../data/raw/application_train.csv'\n",
    "\n",
    "# Bureau table path \n",
    "BUREAU_DATA_PATH = '../data/raw/bureau.csv'\n",
    "\n",
    "# Previous_application table path \n",
    "PREVIOUS_APPLICATION_DATA_PATH = '../data/raw/previous_application.csv'\n",
    "\n",
    "# Interim data directory\n",
    "INTERIM_DIR = '../data/interim'\n",
    "\n",
    "# Processed data directory\n",
    "PROCESSED_DIR = '../data/processed'\n",
    "\n",
    "# Model Directory\n",
    "MODEL_DIR = '../models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(APPLICATION_TRAIN_DATA_PATH)\n",
    "\n",
    "df_bureau = pd.read_csv(BUREAU_DATA_PATH)\n",
    "\n",
    "df_previous_application = pd.read_csv(PREVIOUS_APPLICATION_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the four rows with 'XNA' gender due to insufficient data and imbalance\n",
    "df = df[(df['CODE_GENDER'] == 'F') | (df[\"CODE_GENDER\"] == 'M')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select candidate columns that contain meaningful data and drop others\n",
    "cleaned_df = df.loc[:, ['SK_ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'CODE_GENDER',\n",
    "                    'FLAG_OWN_CAR','FLAG_OWN_REALTY', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL',\n",
    "                    'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'NAME_TYPE_SUITE',\n",
    "                    'NAME_INCOME_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',\n",
    "                    'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
    "                    'OWN_CAR_AGE','OCCUPATION_TYPE', 'CNT_FAM_MEMBERS', 'ORGANIZATION_TYPE',\n",
    "                    'EXT_SOURCE_2', 'EXT_SOURCE_3', 'LIVINGAREA_AVG','TOTALAREA_MODE',\n",
    "                    'OBS_30_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "                    'DEF_60_CNT_SOCIAL_CIRCLE', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
    "                    'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH','FLAG_MOBIL',\n",
    "                    'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE','FLAG_EMAIL',\n",
    "                    'REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY',\n",
    "                    'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
    "                    'LIVE_REGION_NOT_WORK_REGION','DAYS_LAST_PHONE_CHANGE'\n",
    "                  ]\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataframe into numerical data and categorical data\n",
    "cat = cleaned_df.select_dtypes('object')\n",
    "num = cleaned_df.select_dtypes(include=['int64','float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename specific columns related to number of days\n",
    "num = num.rename(columns={\"DAYS_BIRTH\": \"YEARS_BIRTH\",'DAYS_EMPLOYED':'YEARS_EMPLOYED','DAYS_REGISTRATION':'YEARS_REGISTRATION','DAYS_ID_PUBLISH':'YEARS_ID_PUBLISH','DAYS_LAST_PHONE_CHANGE':'YEARS_LAST_PHONE_CHANGE'})\n",
    "\n",
    "# Calculate the number of years for each column in days\n",
    "num['YEARS_BIRTH'] = num['YEARS_BIRTH'].abs()/365\n",
    "num['YEARS_EMPLOYED'] = num['YEARS_EMPLOYED'].abs()/365\n",
    "num['YEARS_REGISTRATION'] = num['YEARS_REGISTRATION'].abs()/365\n",
    "num['YEARS_ID_PUBLISH'] = num['YEARS_ID_PUBLISH'].abs()/365\n",
    "num['YEARS_LAST_PHONE_CHANGE'] = num['YEARS_LAST_PHONE_CHANGE'].abs()/365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the noise values to the median, as the current values are nonsensical.\n",
    "num[\"YEARS_EMPLOYED\"][num[\"YEARS_EMPLOYED\"]>1000] = num[\"YEARS_EMPLOYED\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "num.drop(['AMT_REQ_CREDIT_BUREAU_QRT','FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'YEARS_LAST_PHONE_CHANGE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill five missing values of 'OWN_CAR_AGE' column with the mode value\n",
    "num[(num['OWN_CAR_AGE'].isna()) & (cat[\"FLAG_OWN_CAR\"] == 'Y')] = num[(num['OWN_CAR_AGE'].isna()) & (cat[\"FLAG_OWN_CAR\"] == 'Y')].fillna(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nulls with median for specific columns based on observations and null percentages\n",
    "'''\n",
    "    ['DEF_60_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "    'OBS_30_CNT_SOCIAL_CIRCLE', 'EXT_SOURCE_2', 'AMT_GOODS_PRICE','AMT_ANNUITY']\n",
    "'''\n",
    "\n",
    "num[['DEF_60_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'EXT_SOURCE_2', 'AMT_GOODS_PRICE', 'AMT_ANNUITY', 'AMT_REQ_CREDIT_BUREAU_YEAR']] = num[['DEF_60_CNT_SOCIAL_CIRCLE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'OBS_30_CNT_SOCIAL_CIRCLE', 'EXT_SOURCE_2', 'AMT_GOODS_PRICE', 'AMT_ANNUITY', 'AMT_REQ_CREDIT_BUREAU_YEAR']].fillna(num[['DEF_60_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','OBS_30_CNT_SOCIAL_CIRCLE', 'EXT_SOURCE_2', 'AMT_GOODS_PRICE','AMT_ANNUITY','AMT_REQ_CREDIT_BUREAU_YEAR']].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MissForest(rgr=RandomForestRegressor())\n",
    "num_imputed = mf.fit_transform(num[['LIVINGAREA_AVG', 'TOTALAREA_MODE', 'EXT_SOURCE_3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update num dataframe with the imputed values using the RandomForestAlgorithm\n",
    "num[['LIVINGAREA_AVG']] = num_imputed[['LIVINGAREA_AVG']]\n",
    "num[['TOTALAREA_MODE']] = num_imputed[['TOTALAREA_MODE']]\n",
    "num[['EXT_SOURCE_3']] = num_imputed[['EXT_SOURCE_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([num, cat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application_train = df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application_train['OCCUPATION_TYPE'].fillna(value='XNA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application_train['OWN_CAR_AGE'].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Create a new feature by combining two existing features, \n",
    "calculating an overall rating for a specific region that includes the city. \n",
    "'''\n",
    "\n",
    "df_application_train['OVERALL_REGION_RATING'] = df_application_train['REGION_RATING_CLIENT_W_CITY']/df_application_train['REGION_POPULATION_RELATIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application_train['LOAN_REPAYMENT_PERIOD'] = df_application_train['AMT_CREDIT'] / df_application_train['AMT_ANNUITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_application_train = df_application_train.drop(['CNT_FAM_MEMBERS', 'AMT_CREDIT',\n",
    "                                                'LIVINGAREA_AVG', 'OBS_30_CNT_SOCIAL_CIRCLE',\n",
    "                                                'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
    "                                                'REGION_RATING_CLIENT','LIVE_REGION_NOT_WORK_REGION','NAME_TYPE_SUITE',\n",
    "                                                'REG_REGION_NOT_LIVE_REGION', 'YEARS_REGISTRATION', 'YEARS_ID_PUBLISH', 'REG_REGION_NOT_WORK_REGION'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the row having an anomaly 'AMT_INCOME_TOTAL' \n",
    "df_application_train.drop(df_application_train[df_application_train.AMT_INCOME_TOTAL > 100000000].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Aggregate Secondary Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1 Bureau Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loans = df_bureau.groupby('SK_ID_CURR')['SK_ID_BUREAU'].count().reset_index()\n",
    "total_loans.rename(columns={'SK_ID_BUREAU': 'TOTAL_LOANS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_delay = df_bureau.groupby('SK_ID_CURR')['CREDIT_DAY_OVERDUE'].mean().reset_index()\n",
    "average_delay.rename(columns={'CREDIT_DAY_OVERDUE': 'AVG_REPAYMENT_DELAY'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 Previous_Application Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_previous_loans = df_previous_application.groupby('SK_ID_CURR')['SK_ID_PREV'].count().reset_index()\n",
    "count_previous_loans.rename(columns={'SK_ID_PREV': 'COUNT_PREVIOUS_LOANS'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loan_amount = df_previous_application.groupby('SK_ID_CURR')['AMT_APPLICATION'].mean().reset_index()\n",
    "average_loan_amount.rename(columns={'AMT_APPLICATION': 'AVG_LOAN_AMOUNT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.3 Merging Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging total loans\n",
    "df_application_train = df_application_train.merge(total_loans, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# Merging average repayment delay\n",
    "df_application_train = df_application_train.merge(average_delay, on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging count of previous loans\n",
    "df_application_train = df_application_train.merge(count_previous_loans, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# Merging average loan amount\n",
    "df_application_train = df_application_train.merge(average_loan_amount, on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.4 Cleaning After Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values (if no loans exist for a customer)\n",
    "df_application_train['TOTAL_LOANS'].fillna(0, inplace=True)\n",
    "df_application_train['AVG_REPAYMENT_DELAY'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values (if no previous loans exist for a customer)\n",
    "df_application_train['COUNT_PREVIOUS_LOANS'].fillna(0, inplace=True)\n",
    "df_application_train['AVG_LOAN_AMOUNT'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Utilize a robust scaler since the data contains significant outliers that should not be clipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = df_application_train.select_dtypes(['int64','float64'])\n",
    "categorical = df_application_train.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = list(numerical.columns)\n",
    "categorical_columns = list(categorical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(scaled_data, columns= numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.drop(['SK_ID_CURR', 'REGION_RATING_CLIENT_W_CITY'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.concat([df_scaled, categorical], axis=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
